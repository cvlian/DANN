{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "import gzip\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "old_v = tf.logging.get_verbosity()\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "from tensorflow.python.framework import ops\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "def get_session(gpu_fraction=0.5):\n",
    "    num_threads = os.environ.get('OMP_NUM_THREADS')\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_fraction)\n",
    " \n",
    "    if num_threads:\n",
    "        return tf.Session(config=tf.ConfigProto(\n",
    "            gpu_options=gpu_options, intra_op_parallelism_threads=num_threads))\n",
    "    else:\n",
    "        return tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "    \n",
    "class Dataset:\n",
    "\n",
    "    def getTask(self):\n",
    "        return self.task\n",
    "\n",
    "    def showSamples(self, nrows, ncols):\n",
    "        \"\"\"\n",
    "        Plot nrows x ncols images\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(nrows, ncols)\n",
    "        for i, ax in enumerate(axes.flat): \n",
    "            ax.imshow(self.x[i,:])\n",
    "            ax.set_xticks([]); ax.set_yticks([])\n",
    "            ax.set_title(np.argmax(self.y[i]))\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "class MNISTdata(Dataset):\n",
    "    \"\"\"\n",
    "    MNIST dataset\n",
    "    \n",
    "    A large collection of monochrome images of handwritten digits\n",
    "    \n",
    "    It has a training set of 55,000 examples, and a test set of 10,000 examples\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "        x_train = np.reshape(mnist.train.images, [-1, 28, 28, 1])\n",
    "        x_val = np.reshape(mnist.test.images, [-1, 28, 28, 1])\n",
    "        x_train = np.concatenate([x_train, x_train, x_train], 3)\n",
    "        x_val = np.concatenate([x_val, x_val, x_val], 3)\n",
    "        \n",
    "        print(\"MNIST : Training Set\", x_train.shape)\n",
    "        print(\"MNIST : Test Set\", x_val.shape)\n",
    "        \n",
    "        # Calculate the total number of images\n",
    "        num_images = x_train.shape[0] + x_val.shape[0]\n",
    "        print(\"MNIST : Total Number of Images\", num_images)\n",
    "        \n",
    "        self.task = {'name':'mnist', 'x_train':x_train, 'x_val':x_val, 'y_train':mnist.train.labels, 'y_val':mnist.test.labels}\n",
    "\n",
    "\n",
    "class MNIST_Mdata(Dataset):\n",
    "    \"\"\"\n",
    "    MNIST-M dataset\n",
    "    \n",
    "    This dataset is created by combining MNIST digits with the patches randomly extracted from color photos of BSDS500 as their background\n",
    "    \n",
    "    It contains 55,000 training and 10,000 test images as well\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "        mnistm = pickle.load(open('dataset/mnistm_data.pkl', 'rb'), encoding='latin1')\n",
    "\n",
    "        x_train = np.reshape(mnistm['train'], [-1, 28, 28, 3])/255\n",
    "        x_val = np.reshape(mnistm['test'], [-1, 28, 28, 3])/255\n",
    "        \n",
    "        print(\"MNIST-M : Training Set\", x_train.shape)\n",
    "        print(\"MNIST-M : Test Set\", x_val.shape)\n",
    "\n",
    "        # Calculate the total number of images\n",
    "        num_images = x_train.shape[0] + x_val.shape[0]\n",
    "        print(\"MNIST-M : Total Number of Images\", num_images)\n",
    "        \n",
    "        self.task = {'name':\"mnist-m\", 'x_train':x_train, 'x_val':x_val, 'y_train':mnist.train.labels, 'y_val':mnist.test.labels}\n",
    "\n",
    "\n",
    "class FlipGradientBuilder(object):\n",
    "    \"\"\"\n",
    "    Gradient Reversal Layer (GRL)\n",
    "    \n",
    "    During the forward propagation, GRL acts as an identity transform\n",
    "    \n",
    "    During the backpropagation though, GRL takes the gradient from the subsequent level,\n",
    "    multiplies it by -Î»(learning rate) and pass it to the preceding layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.num_calls = 0\n",
    "\n",
    "    def __call__(self, x, learning_rate=1.0):\n",
    "        \n",
    "        grad_name = \"FlipGradient%d\" % self.num_calls\n",
    "        \n",
    "        @ops.RegisterGradient(grad_name)\n",
    "        def _flip_gradients(op, grad):\n",
    "            return [tf.negative(grad) * learning_rate]\n",
    "        \n",
    "        g = tf.get_default_graph()\n",
    "        with g.gradient_override_map({\"Identity\": grad_name}):\n",
    "            y = tf.identity(x) # copy for assign op\n",
    "            \n",
    "        self.num_calls += 1\n",
    "        return y\n",
    "    \n",
    "def create_placeholders(ht=28, wd=28, ch=3, classes=10):\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "    Arguments:\n",
    "    ht -- height of an input image\n",
    "    wd -- width of an input image\n",
    "    ch -- number of channels of the input\n",
    "    classes -- number of classes, default = 10 (0, 1, ... 9)\n",
    "        \n",
    "    Returns:\n",
    "    Xs, Xt -- data input for source/target tasks\n",
    "    Ys, Yt -- input labels for source/target tasks\n",
    "    D -- domain index (0 = source domain, 1 = target domain)\n",
    "    \"\"\"\n",
    "\n",
    "    Xs = tf.placeholder(tf.float32, [None, ht, wd, ch], name='Xs')\n",
    "    Xt = tf.placeholder(tf.float32, [None, ht, wd, ch], name='Xt')\n",
    "    Ys = tf.placeholder(tf.float32, [None, classes], name='Ys')\n",
    "    Yt = tf.placeholder(tf.float32, [None, classes], name='Yt')\n",
    "    D = tf.placeholder(tf.float32, [None, 2], name='D')\n",
    "    \n",
    "    return Xs, Xt, Ys, Yt, D\n",
    "\n",
    "def initialize_parameters():\n",
    "    \"\"\"\n",
    "    Initializes weight parameters to build a neural network\n",
    "\n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing weight parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    Wf1 = tf.get_variable(\"Wf1\", [3, 3, 3, 32], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    Wf2 = tf.get_variable(\"Wf2\", [3, 3, 32, 64], initializer=tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    Wf3 = tf.get_variable(\"Wf3\", [3, 3, 64, 64], initializer=tf.contrib.layers.xavier_initializer(seed=2))\n",
    "    \n",
    "    bf1 = tf.get_variable(\"bf1\", [32], initializer=tf.zeros_initializer())\n",
    "    bf2 = tf.get_variable(\"bf2\", [64], initializer=tf.zeros_initializer())\n",
    "    bf3 = tf.get_variable(\"bf3\", [64], initializer=tf.zeros_initializer())\n",
    "\n",
    "    Wl1 = tf.get_variable(\"Wl1\", [1024, 256], initializer=tf.contrib.layers.xavier_initializer(seed=4))\n",
    "    Wl2 = tf.get_variable(\"Wl2\", [256, 256], initializer=tf.contrib.layers.xavier_initializer(seed=5))\n",
    "    Wl3 = tf.get_variable(\"Wl3\", [256, 10], initializer=tf.contrib.layers.xavier_initializer(seed=6))\n",
    "\n",
    "    bl1 = tf.get_variable(\"bl1\", [256], initializer=tf.zeros_initializer())\n",
    "    bl2 = tf.get_variable(\"bl2\", [256], initializer=tf.zeros_initializer())\n",
    "    bl3 = tf.get_variable(\"bl3\", [10], initializer=tf.zeros_initializer())\n",
    "\n",
    "    Wd1 = tf.get_variable(\"Wd1\", [1024, 256], initializer=tf.contrib.layers.xavier_initializer(seed=7))\n",
    "    Wd2 = tf.get_variable(\"Wd2\", [256, 256], initializer=tf.contrib.layers.xavier_initializer(seed=8))\n",
    "    Wd3 = tf.get_variable(\"Wd3\", [256, 2], initializer=tf.contrib.layers.xavier_initializer(seed=9))\n",
    "\n",
    "    bd1 = tf.get_variable(\"bd1\", [256], initializer=tf.zeros_initializer())\n",
    "    bd2 = tf.get_variable(\"bd2\", [256], initializer=tf.zeros_initializer())\n",
    "    bd3 = tf.get_variable(\"bd3\", [2], initializer=tf.zeros_initializer())\n",
    "\n",
    "    params = {\"Wf1\": Wf1, \"Wf2\": Wf2, \"Wf3\": Wf3,\n",
    "                  \"bf1\": bf1, \"bf2\": bf2, \"bf3\": bf3,\n",
    "                  \"Wl1\": Wl1, \"Wl2\": Wl2, \"Wl3\": Wl3,\n",
    "                  \"bl1\": bl1, \"bl2\": bl2, \"bl3\": bl3,\n",
    "                  \"Wd1\": Wd1, \"Wd2\": Wd2, \"Wd3\": Wd3,\n",
    "                  \"bd1\": bd1, \"bd2\": bd2, \"bd3\": bd3\n",
    "    }\n",
    "    \n",
    "    return params\n",
    "\n",
    "def forward_propagation(X, params, modelType):\n",
    "    \"\"\"\n",
    "    Forward propagation for the model\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder\n",
    "    params -- dictionary containing weight parameters defined in initialize_parameters\n",
    "    modelType -- choose one among 'feature extractor', 'label predictor', 'domain classifier'\n",
    "\n",
    "    Returns:\n",
    "    Z -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "\n",
    "    Z = X\n",
    "    \n",
    "    if modelType == 'feature extractor' :\n",
    "        for i in range(3) :\n",
    "            Z = tf.nn.conv2d(Z, params['Wf'+str(i+1)], strides=[1, 1, 1, 1], padding='SAME')\n",
    "            Z = tf.nn.relu(Z + params['bf'+str(i+1)])\n",
    "            Z = tf.nn.max_pool(Z, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding='SAME')\n",
    "            Z = tf.nn.dropout(Z, rate=0.05)\n",
    "\n",
    "        Z = tf.contrib.layers.flatten(Z)\n",
    "    elif modelType == 'label predictor' : \n",
    "        for i in range(3) :\n",
    "            Z = tf.matmul(Z, params['Wl'+str(i+1)]) + params['bl'+str(i+1)]\n",
    "\n",
    "            if i == 2 :\n",
    "                break\n",
    "\n",
    "            Z = tf.nn.relu(Z)\n",
    "            Z = tf.nn.dropout(Z, rate=0.05)\n",
    "    elif modelType == 'domain classifier' : \n",
    "        for i in range(3) :\n",
    "            Z = tf.matmul(Z, params['Wd'+str(i+1)]) + params['bd'+str(i+1)]\n",
    "\n",
    "            if i == 2 :\n",
    "                break\n",
    "\n",
    "            Z = tf.nn.relu(Z)\n",
    "            Z = tf.nn.dropout(Z, rate=0.05)\n",
    "    else:\n",
    "        print(\"You have passed a wrong argument\\n\",\n",
    "              \"Model type should be one among 'feature extractor', 'label predictor', 'domain classifier'\")\n",
    "        exit()\n",
    "\n",
    "    return Z\n",
    "\n",
    "def random_mini_batches(X, Y, mini_batch_size=200, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n",
    "    mini_batch_size -- size of the mini-batches, integer\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    m = X.shape[0]                  # number of training examples\n",
    "    mini_batches = []\n",
    "        \n",
    "    # Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation, :]\n",
    "    shuffled_Y = Y[permutation, :].reshape((m, 10))\n",
    "\n",
    "    # Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = int(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[k * mini_batch_size:(k + 1) * mini_batch_size, :]\n",
    "        mini_batch_Y = shuffled_Y[k * mini_batch_size:(k + 1) * mini_batch_size, :]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        end = m - mini_batch_size * int(m / mini_batch_size)\n",
    "        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size:, :]\n",
    "        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size:, :]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches\n",
    "\n",
    "def train(data_S, data_T, training_mode, batch_size=500):\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    Xs, Xt, Ys, Yt, D = create_placeholders(28, 28, 3, 10)\n",
    "    l = tf.placeholder(tf.float32, [], name='l')        # Gradient reversal scaler\n",
    "    params = initialize_parameters()\n",
    "    \n",
    "    Fs = forward_propagation(Xs, params, 'feature extractor')\n",
    "    Ft = forward_propagation(Xt, params, 'feature extractor')\n",
    "    F = tf.concat([Fs, Ft], axis=0)\n",
    "    F_ = _flip_gradient(F, learning_rate=l)\n",
    "    \n",
    "    out_label = forward_propagation(Fs, params, 'label predictor')\n",
    "    out_domain = forward_propagation(F_, params, 'domain classifier')\n",
    "\n",
    "    L_label = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=out_label, labels=Ys))\n",
    "    L_domain = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=out_domain, labels=D))\n",
    "    \n",
    "    L_final = tf.add(L_label, L_domain)\n",
    "\n",
    "    label_train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(L_label)\n",
    "    domain_train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(L_domain)\n",
    "    final_train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(L_final)\n",
    "\n",
    "    y_hat = tf.argmax(tf.nn.softmax(out_label), 1)\n",
    "    d_hat = tf.argmax(tf.nn.softmax(out_domain), 1)\n",
    "\n",
    "    acc_label = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Ys, 1), y_hat), tf.float32))\n",
    "    acc_domain = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(D, 1), d_hat), tf.float32))\n",
    "    \n",
    "    seed = 9449\n",
    "    num_epochs = 100\n",
    "    minibatch_size = 500\n",
    "    \n",
    "    if training_mode == 'dann':\n",
    "        minibatch_size = 250\n",
    "    \n",
    "    sess = tf.compat.v1.InteractiveSession()\n",
    "\n",
    "    # Run the initialization\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "        \n",
    "    # Do the training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        loss_f = 0.; loss_d = 0.; loss_p = 0.\n",
    "        \n",
    "        # number of minibatches of size minibatch_size in the train set\n",
    "        # for the convenience, the size of the learning set and the verification set are the same.\n",
    "        num_batches = int(data_S['x_train'].shape[0] / minibatch_size)\n",
    "        seed = (seed + 2229)%3114\n",
    "        \n",
    "        batches_S = random_mini_batches(data_S['x_train'], data_S['y_train'], mini_batch_size=minibatch_size, seed=seed)\n",
    "        batches_T = random_mini_batches(data_T['x_train'], data_T['y_train'], mini_batch_size=minibatch_size, seed=seed)\n",
    "        \n",
    "        for i, minibatch_S, minibatch_T in zip(range(num_batches), batches_S, batches_T):\n",
    "            # Select a minibatch\n",
    "            (X_s, Y_s) = minibatch_S\n",
    "            (X_t, Y_t) = minibatch_T\n",
    "            \n",
    "            p = float(i) / num_batches\n",
    "            #l_p = 2. / (1. + np.exp(-10. * p)) - 1\n",
    "            l_p = 1.\n",
    "            \n",
    "            if training_mode == 'source only' :\n",
    "                _, _loss_p = sess.run([label_train_op, L_label],\n",
    "                                  feed_dict={'Xs:0': X_s, 'Ys:0': Y_s})                \n",
    "                loss_p = loss_p + _loss_p / num_batches\n",
    "            elif training_mode == 'target only' :\n",
    "                _, _loss_p = sess.run([label_train_op, L_label],\n",
    "                                  feed_dict={'Xs:0': X_t, 'Ys:0': Y_t})                \n",
    "                loss_p = loss_p + _loss_p / num_batches\n",
    "            elif training_mode == 'dann' :\n",
    "                D_ = np.vstack([np.repeat([[1,0]], minibatch_size, axis=0), np.repeat([[0,1]], minibatch_size, axis=0)])\n",
    "        \n",
    "                _, _loss_f, _loss_d, _loss_p = sess.run([final_train_op, L_final, L_domain, L_label],\n",
    "                                                    feed_dict={'Xs:0': X_s, 'Ys:0': Y_s, 'Xt:0': X_t, 'Yt:0': Y_t,\n",
    "                                                               'D:0': D_, 'l:0': l_p})\n",
    "                loss_f = loss_f + _loss_f / num_batches\n",
    "                loss_d = loss_d + _loss_d / num_batches\n",
    "                loss_p = loss_p + _loss_p / num_batches\n",
    "            else :\n",
    "                print(\"You have passed a wrong argument\\n\",\n",
    "                      \"Training method should be one among 'source only', 'target only', 'dann'\")\n",
    "                exit()\n",
    "                \n",
    "        acc_ps, acc_ds = sess.run([acc_label, acc_domain],\n",
    "                                  feed_dict={'Xs:0': data_S['x_val'], 'Ys:0': data_S['y_val'],\n",
    "                                             'Xt:0': np.zeros(shape=(0,28,28,3), dtype=np.float32), 'Yt:0': np.zeros(shape=(0,10), dtype=np.float32),\n",
    "                                             'D:0': np.repeat([[1,0]], data_S['x_val'].shape[0], axis=0)})\n",
    "        acc_pt, acc_dt = sess.run([acc_label, acc_domain],\n",
    "                                  feed_dict={'Xs:0': data_T['x_val'], 'Ys:0': data_T['y_val'],\n",
    "                                             'Xt:0': np.zeros(shape=(0,28,28,3), dtype=np.float32), 'Yt:0': np.zeros(shape=(0,10), dtype=np.float32),\n",
    "                                             'D:0': np.repeat([[0,1]], data_T['x_val'].shape[0], axis=0)})\n",
    "                \n",
    "        report = \"Epoch %i:\\n  âLOSS: \"%(epoch+1)\n",
    "        \n",
    "        if training_mode == 'dann':\n",
    "            report += \"total (%.3f), domain (%.3f), label (%.3f)\\n  \"%(loss_f, loss_d, loss_p)\n",
    "            report += \"âACC: label (S : %.3f/ T : %.3f), domain (S : %.3f/ T : %.3f)\"%(acc_ps, acc_pt, acc_ds, acc_dt)\n",
    "        else :\n",
    "            report += \"label (%.3f)\\n  \"%(loss_p)\n",
    "            report += \"âACC: label (S : %.3f/ T : %.3f)\"%(acc_ps, acc_pt)\n",
    "            \n",
    "        print(report)\n",
    "            \n",
    "mnist = MNISTdata()\n",
    "data_S = mnist.getTask()\n",
    "mnistm = MNIST_Mdata()\n",
    "data_T = mnistm.getTask()\n",
    "\n",
    "get_session(gpu_fraction=0.3)\n",
    "_flip_gradient = FlipGradientBuilder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Model:\n",
    "    \"\"\"\n",
    "    Creates a DANN model that will be trained on the MNIST (source) and MNIST-M (target) dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.build()\n",
    "\n",
    "    def build(self):\n",
    "        tf.reset_default_graph()\n",
    "    \n",
    "        Xs, Xt, Ys, Yt, D = create_placeholders(28, 28, 3, 10)\n",
    "        l = tf.placeholder(tf.float32, [], name='l')        # Gradient reversal scaler\n",
    "        params = initialize_parameters()\n",
    "    \n",
    "        self.Fs = forward_propagation(Xs, params, 'feature extractor')\n",
    "        self.Ft = forward_propagation(Xt, params, 'feature extractor')\n",
    "        self.F = tf.concat([self.Fs, self.Ft], axis=0)\n",
    "        self.F_ = _flip_gradient(self.F, learning_rate=l)\n",
    "    \n",
    "        out_label = forward_propagation(self.Fs, params, 'label predictor')\n",
    "        out_domain = forward_propagation(self.F_, params, 'domain classifier')\n",
    "\n",
    "        self.L_label = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=out_label, labels=Ys))\n",
    "        self.L_domain = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=out_domain, labels=D))\n",
    "    \n",
    "        self.L_final = tf.add(self.L_label, self.L_domain)\n",
    "\n",
    "        self.label_train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(self.L_label)\n",
    "        self.domain_train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(self.L_domain)\n",
    "        self.final_train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(self.L_final)\n",
    "\n",
    "        y_hat = tf.argmax(tf.nn.softmax(out_label), 1)\n",
    "        d_hat = tf.argmax(tf.nn.softmax(out_domain), 1)\n",
    "\n",
    "        self.acc_label = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Ys, 1), y_hat), tf.float32))\n",
    "        self.acc_domain = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(D, 1), d_hat), tf.float32))\n",
    "        \n",
    "    def train(self, data_S, data_T, trainingMode, num_epochs=100, minibatch_size=500):\n",
    "        seed = 9449\n",
    "    \n",
    "        if trainingMode == 'dann':\n",
    "            minibatch_size = minibatch_size//2\n",
    "\n",
    "        sess = tf.compat.v1.InteractiveSession()\n",
    "\n",
    "        # Run the initialization\n",
    "        sess.run(tf.compat.v1.global_variables_initializer())\n",
    "            \n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            loss_f = 0.; loss_d = 0.; loss_p = 0.\n",
    "            \n",
    "            # number of minibatches of size minibatch_size in the train set\n",
    "            # for the convenience, the size of the learning set and the verification set are the same.\n",
    "            num_batches = int(data_S['x_train'].shape[0] / minibatch_size)\n",
    "            seed = (seed + 2229)%3114\n",
    "            \n",
    "            batches_S = random_mini_batches(data_S['x_train'], data_S['y_train'],\n",
    "                                            mini_batch_size=minibatch_size, seed=seed)\n",
    "            batches_T = random_mini_batches(data_T['x_train'], data_T['y_train'],\n",
    "                                            mini_batch_size=minibatch_size, seed=seed)\n",
    "            \n",
    "            for i, minibatch_S, minibatch_T in zip(range(num_batches), batches_S, batches_T):\n",
    "                # Select a minibatch\n",
    "                (X_s, Y_s) = minibatch_S\n",
    "                (X_t, Y_t) = minibatch_T\n",
    "                \n",
    "                p = float(i) / num_batches\n",
    "                #l_p = 2. / (1. + np.exp(-10. * p)) - 1\n",
    "                l_p = 1.\n",
    "                \n",
    "                if trainingMode == 'source only' :\n",
    "                    _, _loss_p = sess.run([self.label_train_op, self.L_label], \n",
    "                                          feed_dict={'Xs:0': X_s, 'Ys:0': Y_s})                \n",
    "                    loss_p = loss_p + _loss_p / num_batches\n",
    "                elif trainingMode == 'target only' :\n",
    "                    _, _loss_p = sess.run([self.label_train_op, self.L_label], \n",
    "                                          feed_dict={'Xs:0': X_t, 'Ys:0': Y_t})                \n",
    "                    loss_p = loss_p + _loss_p / num_batches\n",
    "                elif trainingMode == 'dann' :\n",
    "                    D_ = np.vstack([np.repeat([[1,0]], minibatch_size, axis=0), \n",
    "                                    np.repeat([[0,1]], minibatch_size, axis=0)])\n",
    "            \n",
    "                    _, _loss_f, _loss_d, _loss_p = sess.run([self.final_train_op, self.L_final, self.L_domain, self.L_label],\n",
    "                                                        feed_dict={'Xs:0': X_s, 'Ys:0': Y_s, 'Xt:0': X_t, 'Yt:0': Y_t,\n",
    "                                                                   'D:0': D_, 'l:0': l_p})\n",
    "                    loss_f = loss_f + _loss_f / num_batches\n",
    "                    loss_d = loss_d + _loss_d / num_batches\n",
    "                    loss_p = loss_p + _loss_p / num_batches\n",
    "                else :\n",
    "                    print(\"You have passed a wrong argument\\n\",\n",
    "                        \"Training method should be one among 'source only', 'target only', 'dann'\")\n",
    "                    exit()\n",
    "                    \n",
    "            acc_ps, acc_ds = sess.run([self.acc_label, self.acc_domain],\n",
    "                                feed_dict={'Xs:0': data_S['x_val'], 'Ys:0': data_S['y_val'],\n",
    "                                           'Xt:0': np.zeros(shape=(0,28,28,3), dtype=np.float32), \n",
    "                                           'Yt:0': np.zeros(shape=(0,10), dtype=np.float32),\n",
    "                                           'D:0': np.repeat([[1,0]], data_S['x_val'].shape[0], axis=0)})\n",
    "            acc_pt, acc_dt = sess.run([self.acc_label, self.acc_domain],\n",
    "                                feed_dict={'Xs:0': data_T['x_val'], 'Ys:0': data_T['y_val'],\n",
    "                                           'Xt:0': np.zeros(shape=(0,28,28,3), dtype=np.float32), \n",
    "                                           'Yt:0': np.zeros(shape=(0,10), dtype=np.float32),\n",
    "                                           'D:0': np.repeat([[0,1]], data_T['x_val'].shape[0], axis=0)})\n",
    "                    \n",
    "            report = \"Epoch %i:\\n  âLOSS: \"%(epoch+1)\n",
    "            \n",
    "            if trainingMode == 'dann':\n",
    "                report += \"total (%.3f), domain (%.3f), label (%.3f)\\n  âACC: \"%(loss_f, loss_d, loss_p)\n",
    "            else :\n",
    "                report += \"label (%.3f)\\n  âACC: \"%(loss_p)\n",
    "                \n",
    "            report += \"label (S : %.3f/ T : %.3f), domain (S : %.3f/ T : %.3f)\"%(acc_ps, acc_pt, acc_ds, acc_dt)\n",
    "                \n",
    "            print(report)\n",
    "            \n",
    "        samples = np.random.choice(range(data_S['x_val'].shape[0]), 250, replace=False)\n",
    "        imgs = np.vstack([data_S['x_val'][samples], data_T['x_val'][samples]])\n",
    "        labels = [0]*250 + [1]*250\n",
    "        \n",
    "        f = sess.run(self.Fs, feed_dict={'Xs:0': imgs})\n",
    "        visualize_tsne(f, labels)\n",
    "\n",
    "        sess.close()\n",
    "            \n",
    "def visualize_tsne(x, y):\n",
    "    n = x.shape[0]\n",
    "\n",
    "    model = TSNE(learning_rate=100, n_components=2, random_state=0, n_iter=1000)\n",
    "    transformed = model.fit_transform(x)\n",
    "    \n",
    "    plt.rc('font',family='DejaVu Sans', size=14)\n",
    "    \n",
    "    for v, label in zip(range(2), ['source', 'target']):\n",
    "        idx = [i for i in range(n) if y[i] == v]\n",
    "        plt.scatter(transformed[idx, 0], transformed[idx, 1], label=label)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "        \n",
    "M = Model()\n",
    "        \n",
    "M.train(data_S, data_T, trainingMode='source only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M.train(data_S, data_T, trainingMode='target only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M.train(data_S, data_T, trainingMode='dann')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
